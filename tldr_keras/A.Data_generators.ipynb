{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the data can be too big to load into the memory all at once.For these scenarios we can use data generators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we will be using the `keras.utils.Sequence` as our base class and override the functionality that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-06T09:02:53.225708Z",
     "iopub.status.busy": "2020-06-06T09:02:53.225341Z",
     "iopub.status.idle": "2020-06-06T09:02:53.229902Z",
     "shell.execute_reply": "2020-06-06T09:02:53.229018Z",
     "shell.execute_reply.started": "2020-06-06T09:02:53.225665Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-06T09:01:40.027823Z",
     "iopub.status.busy": "2020-06-06T09:01:40.027611Z",
     "iopub.status.idle": "2020-06-06T09:01:40.032537Z",
     "shell.execute_reply": "2020-06-06T09:01:40.032024Z",
     "shell.execute_reply.started": "2020-06-06T09:01:40.027799Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataGenerator(Sequence):\n",
    "    def __init__(self, id_list, labels, batch_size=32, dim=(32,32,32), n_channels=1, n_classess=10, shuffle=True):\n",
    "        self.dim = dim\n",
    "        self.batch_size=batch_size\n",
    "        self.labels=labels\n",
    "        self.list_IDs=id_list\n",
    "        self.n_channels=n_channels\n",
    "        self.n_classess=n_classess\n",
    "        self.shuffle=shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Updates indexed after each epoch\"\"\"\n",
    "        self.indexes = np.arrange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        \n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        \"\"\"Generates data containing batch_size samples\"\"\" #X:(n_samples, *dim, n_channels)\n",
    "        X = np.empty((self.batch_size, *self.dim, self.channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "        \n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            X[i,] = np.load('path_to_data')\n",
    "            y[i] = self.labels[ID]\n",
    "            \n",
    "        return X, keras.utils.to_categorical(y, num_classess=self.n_classes)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.floor(en(self.list_IDs)/self.batchsize))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(inex+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "        x,y = self.__data_generation(list_IDs_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the method on_epoch_end is triggered once at the very beginning as well as at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the method on_epoch_end is triggered once at the very beginning as well as at the end of each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator's function explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__init__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the constructor where we will accept all the variables that are needed and will store them inside the object for future use.\n",
    "\n",
    "Params accepted:\n",
    "- Labels: the y or target data\n",
    "- batch_size: how much should be the batch stize\n",
    "- dim: the shape/dimension of the input features or the X\n",
    "- n_channels:\n",
    "- n_classes: the no of classes we have for classification\n",
    "- shuffle: if we want to shuffle our data before creating each batch\n",
    "- list_ids: the ids whose data has to be extracted from the X and y variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `on_epoch_end`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is called after the initialization of the Sequence class object and after each epoch. Here we handle the shuffle flag. If its true we need to shuffle the input so that next batch created is shuffled.\n",
    "\n",
    "**Params**: none<br>\n",
    "**returns**: none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-06-06T16:27:33.595348Z",
     "iopub.status.busy": "2020-06-06T16:27:33.595111Z",
     "iopub.status.idle": "2020-06-06T16:27:33.870591Z",
     "shell.execute_reply": "2020-06-06T16:27:33.869126Z",
     "shell.execute_reply.started": "2020-06-06T16:27:33.595321Z"
    }
   },
   "source": [
    "## `__data_generation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function produces the next batch that will be used for the training.\n",
    "\n",
    "Pramas accpeted: list_ids: the list of ids whose X and y values has to be returned\n",
    "If one hot encoded target is needed, then it is converted in this function.\n",
    "If variables are stored in a file and has not been yet read into the memory then the reading of those variables out of the file is handled here.\n",
    "\n",
    "<b>Tip:We can make this function use multiple cores to read data faster.</b>\n",
    "\n",
    "**Params**: list of ids<br>\n",
    "**returns**:X and encoded y in case of one-hot-encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__len__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This denotes the no of batches per epoch. Common practice would be to set this values at:\n",
    "$$\n",
    "\\large\n",
    "\\frac{\\#samples}{batch\\_size}\n",
    "$$\n",
    "\n",
    "**Params**: none<br>\n",
    "**returns**: no of batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `__getitem__`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, once estimator knows the no of batches, it calles the `__getitem__` method to generate the batch.\n",
    "\n",
    "**Params**: index: denotes the batch to fetch<br>\n",
    "**returns**: X and y for the specific batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using it in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_generator=MyDataGenerator(X_train, y, dim:(), batch_size=n, n_classes:c, n_channels:1, shuffle=True)\n",
    "model.fit(generator=my_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf-gpu] *",
   "language": "python",
   "name": "conda-env-tf-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
