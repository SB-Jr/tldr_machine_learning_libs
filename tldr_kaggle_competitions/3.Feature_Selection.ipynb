{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Feature-Selection\" data-toc-modified-id=\"Feature-Selection-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Feature Selection</a></span></li><li><span><a href=\"#Filter-Methods\" data-toc-modified-id=\"Filter-Methods-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Filter Methods</a></span></li><li><span><a href=\"#Wrapper-Methods\" data-toc-modified-id=\"Wrapper-Methods-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Wrapper Methods</a></span></li><li><span><a href=\"#Embedded-Methods\" data-toc-modified-id=\"Embedded-Methods-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Embedded Methods</a></span></li><li><span><a href=\"#Difference-between-Filter-and-Wrapper-Methods\" data-toc-modified-id=\"Difference-between-Filter-and-Wrapper-Methods-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Difference between Filter and Wrapper Methods</a></span></li><li><span><a href=\"#Dimentionality-Reduction-Tehniques\" data-toc-modified-id=\"Dimentionality-Reduction-Tehniques-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Dimentionality Reduction Tehniques</a></span><ul class=\"toc-item\"><li><span><a href=\"#Missing-Value-Ratio-(>20%)\" data-toc-modified-id=\"Missing-Value-Ratio-(>20%)-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Missing Value Ratio (&gt;20%)</a></span></li><li><span><a href=\"#Low-Variance-Filter-(<10%)\" data-toc-modified-id=\"Low-Variance-Filter-(<10%)-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Low Variance Filter (&lt;10%)</a></span></li><li><span><a href=\"#High-Correlation-Filter-(>0.5-0.6)\" data-toc-modified-id=\"High-Correlation-Filter-(>0.5-0.6)-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>High Correlation Filter (&gt;0.5-0.6)</a></span></li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-6.4\"><span class=\"toc-item-num\">6.4&nbsp;&nbsp;</span>Random Forest</a></span></li><li><span><a href=\"#backward-Feature-Elimination\" data-toc-modified-id=\"backward-Feature-Elimination-6.5\"><span class=\"toc-item-num\">6.5&nbsp;&nbsp;</span>backward Feature Elimination</a></span></li><li><span><a href=\"#Forward-Feature-Selection\" data-toc-modified-id=\"Forward-Feature-Selection-6.6\"><span class=\"toc-item-num\">6.6&nbsp;&nbsp;</span>Forward Feature Selection</a></span></li><li><span><a href=\"#Factor-Analysis\" data-toc-modified-id=\"Factor-Analysis-6.7\"><span class=\"toc-item-num\">6.7&nbsp;&nbsp;</span>Factor Analysis</a></span></li><li><span><a href=\"#Principal-Component-Analysis(PCA)\" data-toc-modified-id=\"Principal-Component-Analysis(PCA)-6.8\"><span class=\"toc-item-num\">6.8&nbsp;&nbsp;</span>Principal Component Analysis(PCA)</a></span></li><li><span><a href=\"#Independent-Component-Analysis\" data-toc-modified-id=\"Independent-Component-Analysis-6.9\"><span class=\"toc-item-num\">6.9&nbsp;&nbsp;</span>Independent Component Analysis</a></span></li><li><span><a href=\"#Methods-Based-on-Projections\" data-toc-modified-id=\"Methods-Based-on-Projections-6.10\"><span class=\"toc-item-num\">6.10&nbsp;&nbsp;</span>Methods Based on Projections</a></span></li><li><span><a href=\"#T-Distributed-Stochastic-Neighbour-Embedding(t-SNE)\" data-toc-modified-id=\"T-Distributed-Stochastic-Neighbour-Embedding(t-SNE)-6.11\"><span class=\"toc-item-num\">6.11&nbsp;&nbsp;</span>T-Distributed Stochastic Neighbour Embedding(t-SNE)</a></span></li><li><span><a href=\"#UMAP\" data-toc-modified-id=\"UMAP-6.12\"><span class=\"toc-item-num\">6.12&nbsp;&nbsp;</span>UMAP</a></span></li></ul></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Summary</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "<br>\n",
    "Top reasons to use feature selection are:\n",
    "\n",
    "- It enables the machine learning algorithm to train faster.\n",
    "- It reduces the complexity of a model and makes it easier to interpret.\n",
    "- It improves the accuracy of a model if the right subset is chosen.\n",
    "- It reduces overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='artifacts/filter_1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='artifacts/filter_2.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Pearson’s Correlation:</b> It is used as a measure for quantifying linear dependence between two continuous variables X and Y. Its value varies from -1 to +1. Pearson’s correlation is given as:\n",
    "$$\n",
    "\\rho_{X,Y} = \\frac{cov(X,Y)}{\\sigma_X \\sigma_Y}  \n",
    "$$\n",
    "- <b>LDA:</b> Linear discriminant analysis is used to find a linear combination of features that characterizes or separates two or more classes (or levels) of a categorical variable.\n",
    "- <b>ANOVA:</b> ANOVA stands for Analysis of variance. It is similar to LDA except for the fact that it is operated using one or more categorical independent features and one continuous dependent feature. It provides a statistical test of whether the means of several groups are equal or not.\n",
    "    - <b>Chi-Square:</b> It is a is a statistical test applied to the groups of categorical features to evaluate the likelihood of correlation or association between them using their frequency distribution.<br>\n",
    "<br>\n",
    "One thing that should be kept in mind is that filter methods do not remove multicollinearity. So, you must deal with multicollinearity of features as well before training models for your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='artifacts/wrapper_1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "Some common examples of wrapper methods are forward feature selection, backward feature elimination, recursive feature elimination, etc.\n",
    "\n",
    "- <b>Forward Selection:</b> Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "- <b>Backward Elimination:</b> In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "- <b>Recursive Feature elimination:</b> It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedded Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='artifacts/embedded_1.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods.\n",
    "\n",
    "Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting.\n",
    "\n",
    "- Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "- Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difference between Filter and Wrapper Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between the filter and wrapper methods for feature selection are:\n",
    "\n",
    "- Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "- Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "- Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation.\n",
    "- Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "- Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality Reduction Tehniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value Ratio (>20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can set a threshold so that any feature having missing values more than the threshold can be directly dropped and those below the threshold can be imputed to be used as a feature. Commonly we can set this threshold to 20%, i.e. features having 20% or more missing values can be dropped and those having less can be imputed with the help of other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Variance Filter (<10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features who seldom change in the whole dataset have less variance also. We can then safely assume that the features which rarely change or have low variance, will not add much value to the target prediction. Thus we can safely drop these columns.\n",
    "<br>\n",
    "Its recommended to have a variance threshold of 10% and lower than this, the feature can be dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Correlation Filter (>0.5-0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general guideline, we should keep those variables which show a decent or high correlation with the target variable.\n",
    "\n",
    "<br>\n",
    "High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). We can calculate the correlation between independent numerical variables that are numerical in nature. If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind).\n",
    "<br>\n",
    "<br>\n",
    "If the correlation between a pair of variables is greater than <b>0.5-0.6</b>, we should seriously consider dropping one of those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the data into numeric form by applying one hot encoding, as Random Forest (Scikit-Learn Implementation) takes only numeric inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "from sklearn.ensemble import RndomForestRegressor\n",
    "model = RandomForestReressor(random_state=1, max_depth=10)\n",
    "df = pd.get_dummies(df)\n",
    "model.fit(df, train[target_label])\n",
    "```\n",
    "After fitting\n",
    "```{python}\n",
    "features = df.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above graph, we can hand pick the top-most features to reduce the dimensionality in our dataset.<br> Alernatively, we can use the <b>SelectFromModel</b> of sklearn to do so. It selects the features based on the importance of their weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "feature = SelectFromModel(model)\n",
    "Fit = feature.fit_transform(df, train[target_label])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## backward Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We first take all the n variables present in our dataset and train the model using them\n",
    "- We then calculate the performance of the model\n",
    "- Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables\n",
    "- We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n",
    "- Repeat this process until no variable can be dropped\n",
    "\n",
    "This method can be used when building Linear Regression or Logistic Regression models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the opposite process of the Backward Feature Elimination we saw above. Instead of eliminating features, we try to find the best features which improve the performance of the model. This technique works as follows:\n",
    "\n",
    "- We start with a single feature. Essentially, we train the model n number of times using each feature separately\n",
    "- The variable giving the best performance is selected as the starting variable\n",
    "- Then we repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained\n",
    "- We repeat this process until no significant improvement is seen in the model’s performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Factor Analysis technique, variables are grouped by their correlations, i.e., all variables in a particular group will have a high correlation among themselves, but a low correlation with variables of other group(s). Here, each group is known as a factor. These factors are small in number as compared to the original dimensions of the data. However, these factors are difficult to observe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "FA = FactorAnalysis(n_components = 3).fit_transform(df[feat_cols].values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is a technique which helps us in extracting a new set of variables from an existing large set of variables. These newly extracted variables are called Principal Components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A principal component is a linear combination of the original variables\n",
    "- Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset\n",
    "- Second principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component\n",
    "- Third principal component tries to explain the variance which is not explained by the first two principal components and so on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=4)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use Singular Value Decomposition (SVD) to decompose our original dataset into its constituents, resulting in dimensionality reduction. \n",
    "<br>\n",
    "```{python}\n",
    "from sklearn.decomposition import TruncatedSVD \n",
    "svd = TruncatedSVD(n_components=3, random_state=42).fit_transform(df[feat_cols].values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independent Component Analysis (ICA) is based on information-theory and is also one of the most widely used dimensionality reduction techniques. The major difference between PCA and ICA is that PCA looks for uncorrelated factors while ICA looks for independent factors.\n",
    "\n",
    "If two variables are uncorrelated, it means there is no linear relation between them. If they are independent, it means they are not dependent on other variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm assumes that the given variables are linear mixtures of some unknown latent variables. It also assumes that these latent variables are mutually independent, i.e., they are not dependent on other variables and hence they are called the independent components of the observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{python}\n",
    "from sklearn.decomposition import FastICA \n",
    "ICA = FastICA(n_components=3, random_state=12) \n",
    "X=ICA.fit_transform(df[feat_cols].values)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods Based on Projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By projecting one vector onto the other, dimensionality can be reduced.\n",
    "\n",
    "In projection techniques, multi-dimensional data is represented by projecting its points onto a lower-dimensional space. Now we will discuss different methods of projections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Distributed Stochastic Neighbour Embedding(t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='artifacts/summary.webp'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Missing Value Ratio:</b> If the dataset has too many missing values, we use this approach to reduce the number of variables. We can drop the variables having a large number of missing values in them\n",
    "- <b>Low Variance filter:</b> We apply this approach to identify and drop constant variables from the dataset. The target variable is not unduly affected by variables with low variance, and hence these variables can be safely dropped\n",
    "- <b>High Correlation filter:</b> A pair of variables having high correlation increases multicollinearity in the dataset. So, we can use this technique to find highly correlated features and drop them accordingly\n",
    "- <b>Random Forest:</b> This is one of the most commonly used techniques which tells us the importance of each feature present in the dataset. We can find the importance of each feature and keep the top most features, resulting in dimensionality reduction\n",
    "- <b>Both Backward Feature Elimination and Forward Feature</b> Selection techniques take a lot of computational time and are thus generally used on smaller datasets\n",
    "- <b>Factor Analysis:</b> This technique is best suited for situations where we have highly correlated set of variables. It divides the variables based on their correlation into different groups, and represents each group with a factor\n",
    "- <b>Principal Component Analysis:</b> This is one of the most widely used techniques for dealing with linear data. It divides the data into a set of components which try to explain as much variance as possible\n",
    "- <b>Independent Component Analysis:</b> We can use ICA to transform the data into independent components which describe the data using less number of components\n",
    "- <b>ISOMAP:</b> We use this technique when the data is strongly non-linear\n",
    "- <b>t-SNE:</b> This technique also works well when the data is strongly non-linear. It works extremely well for visualizations as well\n",
    "- <b>UMAP:</b> This technique works well for high dimensional data. Its run-time is shorter as compared to t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
